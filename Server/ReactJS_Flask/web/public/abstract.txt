Transformer is an attention-based model which has achieved state-of-the-art performance for numerous applications based on sequence-to-sequence learning in nature language processing tasks including speech recognition and machine translation. Attention mechanism aims to gathering the features from each token in input sequence to form the output representation. Through extracting and comparing different relations between individual tokens based on self attention, the input sequence is precisely transformed for a sophisticated mapping from source domain to target domain. However, there are twofold weaknesses in learning representation using transformer. First, due to the natural property that attention mechanism would mix up the features of all tokens in input sequence, it is possible that the representation of each token contains redundant information. Second, transformer calculates multiple attention heads in each layer so as to increase the diversity of information exchanges between individual tokens. Such a scheme of multi-head attention likely results in similar patterns in some attention heads. The model capacity using transformer is underestimated. This study presents a new variant of transformer, called the disentangled transformer (DT), to cope with these two weaknesses. The redundancy in self attention using transformer is tackled. Basically, the proposed DT aims to disentangle the representation of input sequences along the horizons of tokens and features of each token embedding vector in sequence. For token-level disentanglement, we propose a cluster-based attention mechanism which carries out the so-called semantic mask attention (SMA). Using SMA, the cluster centroids in each attention layer are used to reflect the subsets of tokens which share similar semantic representation. A mixture of Gaussians is introduced to measure the probability of how similar are the query and key in attention model. This probability acts as a kind of the mask which increases the attention weights for relevant tokens and decreases the attention weights for irrelevant tokens in order to mitigate the issue due to redundant features. On the other hand, this study presents another novel attention mechanism, disentangled head attention, which disentangles attention heads to identify distinct semantic relation between individual token. Similar to SMA, there are several clusters used in each attention layer. However, these clusters are shared between different heads, and only be used during training. The similarity probability between query vector and cluster centroid is used to estimate the mutual information between the query vectors in different heads. By minimizing this mutual information, the diversity of attention patterns between individual heads can be maximized. This objective benefits the performance of attention-based model due to the higher sequence-level understanding. Finally, we combine the semantic mask attention and disentangled head attention as disentangled mask attention and use it as the attention module of DT. With the combination, DT can benefit from the advantages of both attentions. Overall, the proposed DT disentangles input sequences along two different dimensions. The experiments on sequence-to-sequence learning tasks reveal the effectiveness of the proposed methods.