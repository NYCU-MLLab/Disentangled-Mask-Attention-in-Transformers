{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from espnet.utils.io_utils import LoadInputsAndTargets\n",
    "from espnet.asr.pytorch_backend.asr import load_trained_model\n",
    "from espnet.nets.pytorch_backend.disentangled_transformer.add_sos_eos import add_sos_eos\n",
    "from espnet.nets.pytorch_backend.disentangled_transformer.mask import target_mask\n",
    "from espnet.nets.pytorch_backend.disentangled_transformer.attention import DisentangledMaskAttention\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from utils import calculate_redundancy, plot_token_distribution\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", stream=sys.stdout)\n",
    "fontproperties = FontProperties(fname=\"../../../../espnet/nets/pytorch_backend/disentangled_transformer/NotoSansCJK-Regular.ttc\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--model\")\n",
    "parser.add_argument(\"--recog_json\")\n",
    "parser.add_argument(\"--batchsize\", type=int)\n",
    "parser.add_argument(\"--preprocess_conf\", default=None, type=str)\n",
    "\n",
    "tag = \"train_sp_pytorch_disentangled_transformer-6-1024-lr-2\"\n",
    "# tag = \"train_sp_pytorch_transformer-6-256-h-4\"\n",
    "exp_dir = \"/root/Disentangled-Transformer-ASR/egs/aishell/asr1/exp\"\n",
    "ckpt_dir = os.path.join(exp_dir, tag, \"results\")\n",
    "figure_dir = os.path.join(exp_dir, tag, \"figure\")\n",
    "\n",
    "args = parser.parse_args([\n",
    "    \"--model\",\n",
    "    os.path.join(ckpt_dir, \"model.last3.avg.best\"),\n",
    "    \"--recog_json\",\n",
    "    \"/root/Disentangled-Transformer-ASR/egs/aishell/asr1//dump/test/deltafalse/split1utt/data.1.json\",\n",
    "    \"--batchsize\",\n",
    "    \"64\"\n",
    "])\n",
    "\n",
    "if not os.path.exists(figure_dir):\n",
    "    os.makedirs(figure_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:reading a config file from /root/Disentangled-Transformer-ASR/egs/aishell/asr1/exp/train_sp_pytorch_disentangled_transformer-6-1024-lr-2/results/model.json\n",
      "WARNING:root:reading model parameters from /root/Disentangled-Transformer-ASR/egs/aishell/asr1/exp/train_sp_pytorch_disentangled_transformer-6-1024-lr-2/results/model.last3.avg.best\n",
      "INFO:root:encoder self-attention layer type = self-attention\n",
      "INFO:root:decoder self-attention layer type = self-attention\n"
     ]
    }
   ],
   "source": [
    "model, train_args = load_trained_model(args.model)\n",
    "model.eval()\n",
    "load_inputs_and_targets = LoadInputsAndTargets(\n",
    "    mode=\"asr\",\n",
    "    load_output=False,\n",
    "    sort_in_input_length=False,\n",
    "    preprocess_conf=train_args.preprocess_conf\n",
    "    if args.preprocess_conf is None\n",
    "    else args.preprocess_conf,\n",
    "    preprocess_args={\"train\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.recog_json, \"rb\") as f:\n",
    "    js = json.load(f)[\"utts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaulating 7177/7176 ...\n",
      "Layer redundancy: 0.5249\n",
      "Head redundancy: 0.3966\n"
     ]
    }
   ],
   "source": [
    "lr_list = []\n",
    "hr_list = []\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, name in enumerate(js.keys(), 1):\n",
    "        print(\"\\rEvaulating {}/{} ...\".format(idx+1, len(js.keys())), end=\"\")\n",
    "        batch = [(name, js[name])]\n",
    "        feat = load_inputs_and_targets(batch)[0][0]\n",
    "        hs_pad, hs_mask = model.encoder(torch.as_tensor(feat).to(device=device, dtype=torch.float32).unsqueeze(0), None)\n",
    "\n",
    "        tgt = batch[0][1][\"output\"][0][\"tokenid\"]\n",
    "        ys_in_pad, ys_out_pad = add_sos_eos(\n",
    "            torch.as_tensor([int(_tgt) for _tgt in tgt.split(\" \")]).unsqueeze(0),\n",
    "            model.sos, model.eos, model.ignore_id\n",
    "        )\n",
    "        ys_mask = target_mask(ys_in_pad.to(device), model.ignore_id)\n",
    "        pred_pad, pred_mask = model.decoder(ys_in_pad.to(device), ys_mask, hs_pad, hs_mask)\n",
    "\n",
    "        enc_len = hs_pad.size()[1]\n",
    "        dec_len = ys_in_pad.size()[1]\n",
    "\n",
    "        _, lr, hr = calculate_redundancy(model)\n",
    "        \n",
    "        lr_list.append(lr)\n",
    "        hr_list.append(hr)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Layer redundancy: {:.4f}\".format(np.mean(lr_list)))\n",
    "print(\"Head redundancy: {:.4f}\".format(np.mean(hr_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
