# ################################
# Model: HMM Alignment with a Vanilla MLP
# Authors:
# * Elena Rastorgueva 2020
# * Mirco Ravanelli 2020
# * Peter Plantinga 2020
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/augment_noise_CRDNN/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER  # e.g. /path/to/TIMIT
train_annotation: !ref <data_folder>/train.json
valid_annotation: !ref <data_folder>/dev.json
test_annotation: !ref <data_folder>/test.json
skip_prep: False # Skip data prep

# Training parameters
number_of_epochs: 10
batch_size: 256
lr: 0.0003
sorting: ascending # choose between ascending, descending and random
device: 'cuda:0'

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 40

# HMM parameters
states_per_phoneme: 3

# Outputs
phn_set: 60 # {60, 48, 39}
output_neurons: 183
blank_index: 182

# Model parameters
activation: !name:torch.nn.LeakyReLU
dnn_blocks: 1
dnn_neurons: 2000

init_training_type: 'forward' # 'forward' or 'viterbi' or 'ctc'
switch_training_type: 'forward'
switch_training_epoch: 2

dataloader_options:
    batch_size: !ref <batch_size>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
    sample_rate: !ref <sample_rate>
    speeds: [95, 100, 105]

compute_features: !new:speechbrain.lobes.features.Fbank
    context: True
    left_frames: 1
    right_frames: 1
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    n_mels: !ref <n_mels>

model: !new:speechbrain.lobes.models.VanillaNN.VanillaNN
    input_shape: [null, null, !ref <n_mels> * 3]
    dnn_blocks: !ref <dnn_blocks>
    dnn_neurons: !ref <dnn_neurons>
    activation: !ref <activation>

output: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons>
    bias: True

log_softmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

aligner: !new:speechbrain.alignment.aligner.HMMAligner
    states_per_phoneme: !ref <states_per_phoneme>
    batch_reduction: 'mean'
    input_len_norm: True
    target_len_norm: False

opt_class: !name:torch.optim.Adam
    lr: !ref <lr>

lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 5
    annealing_factor: 1.0
    patient: 0

modules:
    model: !ref <model>
    output: !ref <output>
    aligner: !ref <aligner>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        output: !ref <output>
        scheduler: !ref <lr_annealing>
        counter: !ref <epoch_counter>

compute_cost_ctc: !name:speechbrain.nnet.losses.ctc_loss
    blank_index: !ref <blank_index>

compute_cost_nll: !name:speechbrain.nnet.losses.nll_loss

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

accuracy_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !ref <aligner.calc_accuracy>
