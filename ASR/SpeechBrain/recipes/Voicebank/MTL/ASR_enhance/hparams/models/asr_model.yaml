emb_size: 128
dropout_rate: !PLACEHOLDER

# Src embedding parameters
src_activation:
src_dropout: !ref <dropout_rate>
cnn_blocks: 2
cnn_channels: (128, 256)
cnn_kernelsize: (3, 3)
rnn_layers: 4
rnn_neurons: 1024
rnn_bidirectional: True
dnn_blocks: 2
dnn_neurons: 512

# Decoder params
dec_neurons: 1024
dec_rnn_type: gru
dec_attn_type: location
dec_attn_dim: 1024
dec_channels: 10
dec_kernel: 100
dec_layers: 1
dec_dropout: !ref <dropout_rate>

# Language model params
lm_emb_size: 128
lm_dropout: 0.0
lm_rnn_layers: 2
lm_rnn_neurons: 2048
lm_dnn_blocks: 1
lm_dnn_neurons: 512

src_embedding: !new:speechbrain.lobes.models.CRDNN.CRDNN
    input_shape: [null, null, !ref <n_mels>]
    activation: !name:torch.nn.LeakyReLU
    dropout: !ref <src_dropout>
    cnn_blocks: !ref <cnn_blocks>
    cnn_channels: !ref <cnn_channels>
    cnn_kernelsize: !ref <cnn_kernelsize>
    time_pooling: True
    time_pooling_size: 4
    rnn_class: !name:speechbrain.nnet.RNN.LSTM
    rnn_layers: !ref <rnn_layers>
    rnn_neurons: !ref <rnn_neurons>
    rnn_bidirectional: !ref <rnn_bidirectional>
    dnn_blocks: !ref <dnn_blocks>
    dnn_neurons: !ref <dnn_neurons>

tgt_embedding: !new:speechbrain.nnet.embedding.Embedding
    num_embeddings: !ref <output_neurons>
    embedding_dim: !ref <emb_size>

recognizer: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
    rnn_type: !ref <dec_rnn_type>
    attn_type: !ref <dec_attn_type>
    enc_dim: !ref <dnn_neurons>
    input_size: !ref <emb_size>
    hidden_size: !ref <dec_neurons>
    attn_dim: !ref <dec_attn_dim>
    channels: !ref <dec_channels>
    kernel_size: !ref <dec_kernel>
    num_layers: !ref <dec_layers>
    dropout: !ref <dec_dropout>

ctc_output: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons>

seq_output: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dec_neurons>
    n_neurons: !ref <output_neurons>

lm_model: !new:speechbrain.lobes.models.RNNLM.RNNLM
    output_neurons: !ref <output_neurons>
    embedding_dim: !ref <emb_size>
    activation: !name:torch.nn.LeakyReLU
    dropout: !ref <lm_dropout>
    rnn_layers: !ref <lm_rnn_layers>
    rnn_neurons: !ref <lm_rnn_neurons>
    dnn_blocks: !ref <lm_dnn_blocks>
    dnn_neurons: !ref <lm_dnn_neurons>
    return_hidden: True
