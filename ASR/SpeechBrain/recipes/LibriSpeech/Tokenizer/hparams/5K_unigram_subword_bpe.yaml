# ############################################################################
# Tokenizer: subword BPE with unigram 5K
# Training: Librispeech 960h
# Authors:  Abdel Heba 2021
# ############################################################################

output_folder: !ref results/5K_subword_unigram_960h_LM/
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER # e.g., /path/to/LibriSpeech
train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev-clean.csv

# Training parameters
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 5000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: wrd
bos_index: 1
eos_index: 2


tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: wrd
   model_type: "unigram" # ["unigram", "bpe", "char"]
   character_coverage: 1.0
   bos_id: !ref <bos_index> # Define bos_id/eos_id if different from blank_id
   eos_id: !ref <eos_index>
   annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
